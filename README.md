# ğŸ™ï¸ VoiceCommand API - Intelligent Field Reporting

> **Architecture Microservices hybride (Node.js + Python) pour la structuration automatique de comptes rendus de chantier par IA.**

![Status](https://img.shields.io/badge/Status-Prototype-orange)
![Docker](https://img.shields.io/badge/Docker-Enabled-blue)
![Stack](https://img.shields.io/badge/Tech-NodeJS%20|%20FastAPI%20|%20Whisper%20|%20Gemini-green)

## ğŸ“– Ã€ propos

**VoiceCommand** rÃ©sout un problÃ¨me critique pour les travailleurs de terrain (Architectes, BTP) : la lourdeur administrative des rapports.

L'API transforme une note vocale brute et informelle (ex: *"Le mur est fissurÃ©, faut voir Ã§a vendredi"*) en :

1.  **DonnÃ©es structurÃ©es (JSON)** : Pour l'intÃ©gration automatique dans les ERP/outils de gestion.
2.  **Rapport formatÃ© (Markdown)** : Un compte rendu professionnel, corrigÃ© et prÃªt Ã  l'envoi par email.

## ğŸ—ï¸ Architecture Technique

Ce projet implÃ©mente une architecture **Microservices** pour dÃ©coupler la gestion des requÃªtes HTTP du traitement lourd de l'IA.

* **Gateway Service (Node.js/Express)** : GÃ¨re l'upload, la validation des fichiers, et la sÃ©curitÃ©. OptimisÃ© pour les I/O asynchrones.
* **AI Engine Service (Python/FastAPI)** : GÃ¨re le pipeline d'intelligence artificielle.
    * **Perception** : `faster-whisper` (implÃ©mentation CTranslate2) pour une transcription locale ultra-rapide sur CPU.
    * **Cognition** : `Google Gemini 2.5 Flash` pour l'extraction d'entitÃ©s (Dates, Lots, TÃ¢ches) et la gÃ©nÃ©ration du rapport.

## ğŸš€ Stack Technologique

| Composant | Technologie | Justification |
| :--- | :--- | :--- |
| **Orchestration** | **Docker Compose** | Environnement iso-prod, rÃ©plicable en une commande. |
| **Gateway** | **Node.js (Express)** | Gestion efficace des flux de fichiers (Multer) et faible latence rÃ©seau. |
| **AI Backend** | **Python (FastAPI)** | Standard de l'industrie pour le ML/AI. |
| **Transcription** | **Faster-Whisper** | 4x plus rapide que Whisper standard, permet l'infÃ©rence CPU (`int8`). |
| **LLM** | **Gemini 2.5 Flash** | FenÃªtre de contexte large, rapide et Ã©conomique pour le "Structured Output". |

## ğŸ› ï¸ Installation & DÃ©marrage

**PrÃ©requis :** Docker & Docker Compose installÃ©s, une clÃ© API Google AI Studio.

### 1. Cloner le projet
```bash
git clone [https://github.com/votre-user/voice-command-api.git](https://github.com/votre-user/voice-command-api.git)
cd voice-command-api
````

### 2\. Configuration des variables d'environnement

Dupliquez le fichier d'exemple et ajoutez votre clÃ© API.

```bash
cp .env.example .env
```

Ouvrez `.env` et insÃ©rez votre clÃ© :

```env
GOOGLE_API_KEY=votre_cle_api_ici
AI_SERVICE_URL=http://voice_ai_engine:8000
```

### 3\. Lancer l'architecture

```bash
docker compose up --build
```

*Note : Le premier lancement peut prendre 1-2 minutes le temps de tÃ©lÃ©charger les images Docker et le modÃ¨le Whisper (Small).*

## ğŸ”Œ Utilisation de l'API

L'API Gateway Ã©coute sur le port `3000`.

### Endpoint : Traitement Audio

**POST** `/process-voice`

| ParamÃ¨tre | Type | Description |
| :--- | :--- | :--- |
| `audio` | File (Form-Data) | Le fichier audio (.mp3, .wav, .m4a) |

### Exemple Curl

```bash
curl -X POST http://localhost:3000/process-voice \
  -F "audio=@/chemin/vers/votre/fichier.mp3"
```

### Exemple de RÃ©ponse (JSON)

```json
{
  "success": true,
  "data": {
    "raw_transcription": "Le mur porteur est fissurÃ©...",
    "structured_report": {
      "project_name": "RÃ©novation Rue de la RÃ©publique",
      "date": "2024-12-10",
      "trades": [
        {
          "trade_name": "MaÃ§onnerie",
          "tasks": [
            {
              "description": "Reprise fissure mur porteur (enduit fibrÃ©)",
              "status": "Non conforme",
              "deadline": "2024-12-13"
            }
          ]
        }
      ],
      "formatted_report": "**Compte Rendu de Chantier**\n\nBonjour, voici le relevÃ©..."
    }
  }
}
```

## ğŸ“‚ Structure du Projet

```text
.
â”œâ”€â”€ docker-compose.yml      # Orchestration des services
â”œâ”€â”€ .env.example            # Template des secrets
â”œâ”€â”€ api-gateway/            # Service Node.js
â”‚   â”œâ”€â”€ index.js            # Point d'entrÃ©e & Routing
â”‚   â”œâ”€â”€ Dockerfile          # Image Node Alpine
â”‚   â””â”€â”€ uploads/            # Stockage temporaire (non-gitÃ©)
â””â”€â”€ ai-engine/              # Service Python
    â”œâ”€â”€ main.py             # Logique FastAPI & Pipeline AI
    â”œâ”€â”€ Dockerfile          # Image Python Slim + dÃ©pendances systÃ¨me
    â””â”€â”€ requirements.txt    # Libs Python (FastAPI, Whisper, GoogleGenAI)
```

## ğŸ”® Roadmap & AmÃ©liorations Futures

Ce projet est un MVP fonctionnel. Pour passer Ã  l'Ã©chelle (Production), les prochaines Ã©tapes sont :

1.  **Queue Asynchrone (Redis/BullMQ)** : Pour ne pas bloquer la requÃªte HTTP pendant le traitement IA (actuellement synchrone).
2.  **Stockage Cloud (S3)** : Remplacer le stockage local temporaire pour supporter le scaling horizontal.
3.  **SÃ©curitÃ©** : Ajouter une authentification JWT sur l'API Gateway.
4.  **CI/CD** : Pipeline GitHub Actions pour les tests automatiques et le linting.

-----

**Auteur** : Maxime Kirch
**Contact** : [maxime.kirch@gmail.com](mailto:maxime.kirch@gmail.com) | [LinkedIn](https://www.linkedin.com/in/maxime-kirch/)
